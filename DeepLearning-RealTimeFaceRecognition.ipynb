{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time face recognizition application using deep neural network \n",
    "\n",
    "\n",
    "Below is the implementation of face detector and recognizer which can identify the face of the person showing on a web cam. We'll be implementing it Keras framework.\n",
    "\n",
    "The deep neural nework we'll be using here is based on [FaceNet](https://arxiv.org/pdf/1503.03832.pdf), which was published by Google in 2015 and achieved 99.57% accuracy on a popular face recognition dataset named â€œLabeled Faces in thae Wild(LFW)\". You can find its open-source Keras version [here](https://github.com/iwantooxxoox/Keras-OpenFace) and Tensorflow version [here](https://github.com/davidsandberg/facenet), and play around to build your own models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mtcnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b7d5817e7392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtcnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mtcnn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "import utils\n",
    "#keras imported in utils.py file\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to let computers tell whether two pictures are the same person?\n",
    "\n",
    "Looking at the two photos below with our naked eyes, we can easily tell it is the same person, although the hairstyle, dressing and distance from the camera are different. But how can we let computers tell whether it is the same person or not?\n",
    "\n",
    "![1](pictures/yifei.jpg) \n",
    "\n",
    "Notice when computers 'see' pictures, a RGB picture will be 'seen' as values with RGB three channels at each pixel of the picture. If it is a pixel_size*pixel_size RGB picture, it will be a (pixel_size, pixel_size, 3) matrix. \n",
    "\n",
    "Then, how to let computers tell whether two matrices represent the same person? At first, we might think of reshaping the (pixel_size, pixel_size, 3) matrix into a 1-dimensional vector and verify whether they are the same person based on the distance between them. However, the time when she took different pictures, she might be dressing in different clothes, wearing different accessories, standing at different distances away from the camera, etc. All these possibilities will significantly mislead the computer's judgements. \n",
    "\n",
    "Based on this, a direct comparation of corresponding 1-d vectors of two pictures is not an ideal strategy. Instead, we'll approach this problem by encoding the input picture into a 128-dimentional embedding by passing this picture through a deep neural network, and use the 128-dimentional embedding as the representaion of each picture. The model architecture is shown below. If the distance between two 128-d vectors is larger than the customized threshold, then these two pictures are not the same person, vice versa. We'll talk about the triplet loss function in later chapter, first, let's implement the deep neural network. \n",
    "![2](pictures/model.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network--Facenet\n",
    "\n",
    "Notice the input data shape is (96, 96, 3), which is 96*96 pixel RGB(3 channels) picture; after driving through this Inception-blocks model, the last layer (which is the output) is a fully connected layer with 128 neurons. The output 128-dimension vector extracts the important features of the input facial picture and will be as the representaion of input picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import facenet model\n",
    "#see inception_blocks.py for model implementation\n",
    "from utils import LRN2D\n",
    "import utils\n",
    "from inception_blocks import *\n",
    "\n",
    "#show the architecture of the network\n",
    "model = faceRecoModel((96, 96, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet loss function\n",
    "\n",
    "The FaceNet model converts input images into 128-d embeddings to represent the image. Then parameters are trained by minimizing the triplet loss. The Triplet Loss minimizes the distance between an anchor and a positive, both of which have the same identity, and maximizes the distance between the anchor and a negative of a different identity. As shown below:\n",
    "\n",
    "![3](pictures/triplet.png)\n",
    "\n",
    "The training process requires GPU and high amount of training data, you can also transfer learning and fine tune the weights. But here we'll be loading previously trained weights, which are available at [here](https://github.com/iwantooxxoox/Keras-OpenFace)  in the \"weights\" folder and they are also provided in this source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights(this process will take a few minutes)\n",
    "import utils\n",
    "weights = utils.weights\n",
    "weights_dict = utils.load_weights()\n",
    "\n",
    "for name in weights:\n",
    "  if model.get_layer(name) != None:\n",
    "    model.get_layer(name).set_weights(weights_dict[name])\n",
    "  elif model.get_layer(name) != None:\n",
    "    model.get_layer(name).set_weights(weights_dict[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture, crop, align and resize identity face image in real time using OpenCV\n",
    "In this section, we'll be using OpenCV (make sure you've installed it) to open a web camera, detect and outine the face area using a blue rectangle and then capture 15 face images of the person that is in front of the camera. \n",
    "\n",
    "These cropped face snapshots are stored in **\"images\"** folder with the name NameHere_1 to NameHere_15. Select onely one well captured face image from these 15 images for each person. Rename it with the name of person and delete rest of them. Repeat this process by different people, with each person only keeps one picture in this folder. Later in this program, when a person shows up in front of the camera, it will calculate its distance from each stored pictures and return the most likely one's name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "count = 0\n",
    "while(True):\n",
    "    # capture frame by frame\n",
    "    ret, img = cap.read()\n",
    "    \n",
    "    # detect the face, you can change the scaleFactor according to your case\n",
    "    faces = detector.detectMultiScale(img, scaleFactor= 1.5, minNeighbors= 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        \n",
    "        # outline the face area by a blue rectangle\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0),2)     \n",
    "        count += 1\n",
    "        # save the cropped face image into the datasets folder\n",
    "        cv2.imwrite(\"images/NameHere_\" + str(count) + \".jpg\", img[y:y+h,x:x+w])\n",
    "        cv2.imshow('image', img)\n",
    "    # Press 'ESC' for exiting video\n",
    "    k = cv2.waitKey(200) & 0xff \n",
    "    if k == 27:\n",
    "        break\n",
    "    elif count >= 8: \n",
    "         break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or detect face by Multi-task CNN\n",
    "Besides using OpenCV to detect face, we can also use Dlib or deep learning Multi-task CNN. Here we show how to use MTCNN to detect the face from an image. After running this section, you may go to \"pictures\" folder to check t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mtcnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b27584ac60aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtcnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pictures/yifei.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdetector1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdetector1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mtcnn'"
     ]
    }
   ],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "image= cv2.imread('pictures/yifei.jpg')\n",
    "detector1= MTCNN()\n",
    "result=detector1.detect_faces(image)\n",
    "print(result)\n",
    "\n",
    "count=0\n",
    "for person in result:\n",
    "            bounding_box = person['box']\n",
    "            x=bounding_box[0]\n",
    "            y=bounding_box[1]\n",
    "            w=bounding_box[2]\n",
    "            h=bounding_box[3]\n",
    "            keypoints = person['keypoints']\n",
    "            \n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (255,0,255), 2)\n",
    "            cv2.circle(image,(keypoints['left_eye']), 2, (0,155,255), 2)\n",
    "            cv2.circle(image,(keypoints['right_eye']), 2, (0,155,255), 2)\n",
    "            cv2.circle(image,(keypoints['nose']), 2, (0,155,255), 2)\n",
    "            cv2.circle(image,(keypoints['mouth_left']), 2, (0,155,255), 2)\n",
    "            cv2.circle(image,(keypoints['mouth_right']), 2, (0,155,255), 2)\n",
    "            cv2.imwrite(\"pictures/\" + str(count)+ \"_detected.jpg\", image)\n",
    "            cv2.imwrite(\"pictures/\" + str(count)+ \".jpg\", image[y:y+h,x:x+w])\n",
    "            count +=1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to recognize faces:\n",
    "#First, encode one single image into embeddings\n",
    "#Second, build a database containing embeddings for all images by passing all images through the weighted Facenet model\n",
    "#Third, identify images by using the embeddings(find the minimum L2 euclidean distance between embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, encode one single image into embeddings\n",
    "def image_to_embedding(image, model):\n",
    "    image = cv2.resize(image, (96, 96)) \n",
    "    img = image[...,::-1]\n",
    "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "\n",
    "#Second, build a database containing embeddings for all images\n",
    "def build_database_dict():\n",
    "    database = {}   \n",
    "    for file in glob.glob(\"/Users/Olivia/Documents/ML/Face-recognition-using-deep-learning-master/images/*\"):\n",
    "        database_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        image_file = cv2.imread(file, 1)\n",
    "        database[database_name] = image_to_embedding(image_file, model)\n",
    "    return database\n",
    "\n",
    "\n",
    "#Third, identify images by using the embeddings(find the minimum L2 euclidean distance between embeddings)\n",
    "def recognize_face(face_image, database, model):\n",
    "    \n",
    "    embedding = image_to_embedding(face_image, model)   \n",
    "    minimum_distance = 200\n",
    "    name = None    \n",
    "    # Loop over  names and encodings.\n",
    "    for (database_name, database_embedding) in database.items():\n",
    "            \n",
    "        euclidean_distance = np.linalg.norm(embedding-database_embedding)\n",
    "        print('Euclidean distance from %s is %s' %(database_name, euclidean_distance))\n",
    "        if euclidean_distance < minimum_distance:\n",
    "            minimum_distance = euclidean_distance\n",
    "            name = database_name\n",
    "    \n",
    "    if minimum_distance < 0.8:\n",
    "        return str(name)+str('  ')+str(round(minimum_distance,14))\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bdb24768c8d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbuild_database_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/Obama.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrecognize_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "database= build_database_dict()\n",
    "image= cv2.imread('images/Obama.jpg')\n",
    "recognize_face(image, database, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize faces in real time using webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-905dcd414eb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mface_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0midentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecognize_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midentity\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.namedWindow(\"Face Recognizer\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "      \n",
    "while True:\n",
    "    ret, frame = vc.read()\n",
    "    height, width, channels = frame.shape\n",
    "     \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # loop through all the faces detected \n",
    "    for (x, y, w, h) in faces:          \n",
    "        face_image = frame[max(0, y):min(height, y+h), max(0, x):min(width, x+w)]    \n",
    "        identity = recognize_face(face_image, database, model)          \n",
    "        if identity is not None:\n",
    "            img = cv2.rectangle(frame,(x, y),(x+w, y+h),(255,0,0),2)\n",
    "            cv2.putText(frame, str(identity), (x+5,y-5), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,255), 2)\n",
    "        \n",
    "    key = cv2.waitKey(100)\n",
    "    cv2.imshow(\"Face Recognizer\", frame)\n",
    "    \n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "vc.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
